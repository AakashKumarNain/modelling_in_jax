# Building Models in Pure JAX

> **Build production-ready ML models in pure JAX with minimal abstractions that scale to thousands of accelerators**

[![JAX](https://img.shields.io/badge/JAX-Latest-orange.svg)](https://github.com/google/jax)
[![Python](https://img.shields.io/badge/Python-3.11%2B-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-Apache--2.0-green.svg)](LICENSE)

## Motivation

This repository is your guide to mastering **pure JAX** for building scalable ML models without any third-party frameworks. You'll learn to create minimal yet powerful abstractions that can efficiently run on massive accelerator clusters.

### Objectives

- **Pure JAX Implementation**: Build models using only JAX, no external ML libraries
- **Minimal Abstractions**: Design clean, scalable patterns for distributed training
- **Complement JAX Scaling**: Enhance your knowledge from the official JAX scaling documentation
- **(M)LLM Focus**: Apply these patterns specifically to (MultiModal) Large Language Models


## Target Audience

This repository is designed for:
- **Expert ML Engineers** who is already aware of building models in PyTorch or JAX, but want more depth
- **ML Researchers** who are eager to try building models in JAX
- **Performance Engineers** optimizing for scale
  **Anyone** seeking deep understanding of JAX's capabilities

## Table of Contents

I work on this project in my free time between my day job, personal research, annotating new research papers, and contributing to open source. Expect slow to very-slow progress. Here is the list of things we will cover:

### Core Concepts
- [ ] **When to Follow These Patterns**
  - Identifying use cases for pure JAX
  - This vs. high-level frameworks (e.g. Equinox, Keras)
  - Performance and flexibility considerations

- [ ] **Building a Simple Layer**
  - Parameter initialization and management
  - Forward and backward pass implementation

- [ ] **Minimal Abstractions for Scale**
  - Essential patterns for distributed training
  - Memory efficient implementations
  - Keeping abstractions lightweight yet powerful

### Advanced Topics *(Coming Soon)*
- [ ] Multi-host training strategies
- [ ] Memory optimization techniques
- [ ] Custom gradient transformations
- [ ] Profiling and debugging at scale

## Complementary Resources

This repository works best alongside:
- [JAX Scaling Guide](https://jax.readthedocs.io/en/latest/distributed_arrays_and_automatic_parallelization.html)
- [JAX Documentation](https://jax.readthedocs.io/)
- [XLA Performance Guide](https://www.tensorflow.org/xla/performance)


## Contributing

We welcome contributions from the community! Whether it's:
- Bug fixes and improvements
- Documentation enhancements  
- New patterns and examples
- Performance optimizations

## ⚠️ Note

This is an **advanced** resource. We assume familiarity with:
- JAX fundamentals and transformations
- Distributed computing concepts (DDP, FSDP, TP, EP, etc.)
- Neural network architectures
- Production ML workflows
---
